{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a20c1ff-b5ec-4ab0-bbe9-44640eec6084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 1: 0 unknown → 0 reassigned, 0 removed.\n",
      "Phase 2: Inserted 0 continuity rows.\n",
      "Phase 3: No IDs merged.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 102\u001b[0m\n\u001b[1;32m    100\u001b[0m             removed_outliers \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    101\u001b[0m     cleaned\u001b[38;5;241m.\u001b[39mappend(grp[keep])\n\u001b[0;32m--> 102\u001b[0m clean_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcleaned\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPhase 4: Removed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mremoved_outliers\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m outlier jumps.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# Final output to (frame, old_id, id, x, y)\u001b[39;00m\n",
      "File \u001b[0;32m~/detectron_env/lib/python3.8/site-packages/pandas/core/reshape/concat.py:372\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    370\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 372\u001b[0m op \u001b[38;5;241m=\u001b[39m \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[0;32m~/detectron_env/lib/python3.8/site-packages/pandas/core/reshape/concat.py:429\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    426\u001b[0m     objs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 429\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    432\u001b[0m     objs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs))\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "#cleaner.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Load the movement log\n",
    "# Ensure your CSV has columns: frame, id, x, y\n",
    "input_csv = 'movement_log.csv'\n",
    "output_csv = 'movement_log_final1.csv'\n",
    "\n",
    "# Parameters\n",
    "merge_distance_threshold = 20      # pixels: merge based on overall-centroid proximity\n",
    "outlier_distance_threshold = 50   # pixels: max allowed frame-to-frame jump\n",
    "\n",
    "# Read data\n",
    "orig_df = pd.read_csv(input_csv)\n",
    "df = orig_df.copy()\n",
    "\n",
    "# Phase 1: Reassign unknowns → nearest known centroid\n",
    "df['old_id'] = df['id']\n",
    "def compute_centroids(data):\n",
    "    known = data[data['old_id'] != 'unknown']\n",
    "    return known.groupby('old_id')[['x','y']].mean().to_dict('index')\n",
    "centroids = compute_centroids(df)\n",
    "\n",
    "total_unknown = (df['old_id'] == 'unknown').sum()\n",
    "def assign_nearest(row, cents):\n",
    "    if row['old_id'] == 'unknown' and cents:\n",
    "        best, min_d = None, float('inf')\n",
    "        for k, coord in cents.items():\n",
    "            d = math.hypot(row['x']-coord['x'], row['y']-coord['y'])\n",
    "            if d < min_d:\n",
    "                best, min_d = k, d\n",
    "        return best if best is not None else 'unknown'\n",
    "    return row['old_id']\n",
    "\n",
    "df['id_assigned'] = df.apply(lambda r: assign_nearest(r, centroids), axis=1)\n",
    "df = df[df['id_assigned'] != 'unknown'].copy()\n",
    "\n",
    "assigned_unknown = total_unknown - ((orig_df['id'] == 'unknown') & (df['id_assigned'] == 'unknown')).sum()\n",
    "print(f\"Phase 1: {total_unknown} unknown → {assigned_unknown} reassigned, {total_unknown - assigned_unknown} removed.\")\n",
    "\n",
    "# Phase 2: Continuity correction\n",
    "all_frames = sorted(orig_df['frame'].unique())\n",
    "x_min, x_max = df['x'].min(), df['x'].max()\n",
    "y_min, y_max = df['y'].min(), df['y'].max()\n",
    "continuity_rows = []\n",
    "for pid, group in df.groupby('id_assigned'):\n",
    "    positions = dict(zip(group['frame'], zip(group['x'], group['y'])))\n",
    "    for f in all_frames:\n",
    "        if f not in positions:\n",
    "            prev_frames = [pf for pf in positions if pf < f]\n",
    "            next_frames = [nf for nf in positions if nf > f]\n",
    "            if prev_frames and next_frames:\n",
    "                pf = max(prev_frames)\n",
    "                nf = min(next_frames)\n",
    "                px, py = positions[pf]\n",
    "                nx, ny = positions[nf]\n",
    "                if x_min <= px <= x_max and y_min <= py <= y_max and x_min <= nx <= x_max and y_min <= ny <= y_max:\n",
    "                    continuity_rows.append({\n",
    "                        'frame': f,\n",
    "                        'old_id': pid,\n",
    "                        'id_assigned': pid,\n",
    "                        'x': px,\n",
    "                        'y': py\n",
    "                    })\n",
    "inserted = len(continuity_rows)\n",
    "if continuity_rows:\n",
    "    df = pd.concat([df, pd.DataFrame(continuity_rows)], ignore_index=True)\n",
    "print(f\"Phase 2: Inserted {inserted} continuity rows.\")\n",
    "\n",
    "# Phase 3: Merge close ID clusters based on overall centroids\n",
    "centroids2 = df.groupby('id_assigned')[['x','y']].mean().to_dict('index')\n",
    "ids = list(centroids2.keys())\n",
    "merge_map = {}\n",
    "for i, id1 in enumerate(ids):\n",
    "    for id2 in ids[i+1:]:\n",
    "        d = math.hypot(centroids2[id1]['x'] - centroids2[id2]['x'], \n",
    "                       centroids2[id1]['y'] - centroids2[id2]['y'])\n",
    "        if d < merge_distance_threshold:\n",
    "            merge_map[id2] = id1\n",
    "\n",
    "df['id_merged'] = df['id_assigned'].apply(lambda x: merge_map.get(x, x))\n",
    "if merge_map:\n",
    "    print(\"Phase 3: Merged the following IDs:\")\n",
    "    for src, dst in merge_map.items():\n",
    "        print(f\"  - ID {src} → ID {dst}\")\n",
    "else:\n",
    "    print(\"Phase 3: No IDs merged.\")\n",
    "\n",
    "# Phase 4: Remove outliers by frame-to-frame jump\n",
    "cleaned = []\n",
    "removed_outliers = 0\n",
    "for pid, group in df.groupby('id_merged'):\n",
    "    grp = group.sort_values('frame')\n",
    "    keep = [True] * len(grp)\n",
    "    coords = list(zip(grp['x'], grp['y']))\n",
    "    for i in range(1, len(coords)):\n",
    "        if math.hypot(coords[i][0] - coords[i-1][0], coords[i][1] - coords[i-1][1]) > outlier_distance_threshold:\n",
    "            keep[i] = False\n",
    "            removed_outliers += 1\n",
    "    cleaned.append(grp[keep])\n",
    "clean_df = pd.concat(cleaned)\n",
    "print(f\"Phase 4: Removed {removed_outliers} outlier jumps.\")\n",
    "\n",
    "# Final output to (frame, old_id, id, x, y)\n",
    "def try_int(v):\n",
    "    try: return int(v)\n",
    "    except: return v\n",
    "\n",
    "final_df = clean_df.copy()\n",
    "final_df['old_id'] = final_df['old_id'].apply(try_int)\n",
    "final_df['id']     = final_df['id_merged'].apply(try_int)\n",
    "\n",
    "final_df = ( final_df\n",
    "    .loc[:, ['frame','old_id','id','x','y']]\n",
    "    .sort_values(['frame','id'])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "final_df.to_csv(output_csv, index=False)\n",
    "print(f\"Final cleaned log saved to '{output_csv}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461c8677-d54a-4e81-a737-a2f129a689c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# Parameters\n",
    "video_path = 'video/Ev1_2mp4.mp4'\n",
    "csv_path = 'movement_log_final.csv'  # processed log with x,y coordinates\n",
    "\n",
    "# Output filenames\n",
    "output_frame_image = 'heatmap_overlay_frame.png'\n",
    "output_white_image = 'heatmap_overlay_white.png'\n",
    "\n",
    "# ✅ Increase these numbers to make grid smaller (finer)\n",
    "grid_rows = 20  # number of vertical divisions\n",
    "grid_cols = 20  # number of horizontal divisions\n",
    "\n",
    "alpha = 0.6    # weight for background\n",
    "beta = 0.4     # weight for heatmap\n",
    "\n",
    "\n",
    "def create_heatmap_overlay():\n",
    "    # Read first frame to get dimensions\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    success, frame = cap.read()\n",
    "    cap.release()\n",
    "    if not success:\n",
    "        raise IOError(f\"Cannot read frame 1 from {video_path}\")\n",
    "    height, width = frame.shape[:2]\n",
    "\n",
    "    # Grid cell dimensions\n",
    "    zone_h = height // grid_rows\n",
    "    zone_w = width // grid_cols\n",
    "\n",
    "    # Read movement log and count visits per cell\n",
    "    df = pd.read_csv(csv_path)\n",
    "    counts = np.zeros((grid_rows, grid_cols), dtype=int)\n",
    "    for _, row in df.iterrows():\n",
    "        x, y = int(row['x']), int(row['y'])\n",
    "        r = min(y // zone_h, grid_rows - 1)\n",
    "        c = min(x // zone_w, grid_cols - 1)\n",
    "        counts[r, c] += 1\n",
    "\n",
    "    # Normalize counts to [0, 255] for heatmap intensity\n",
    "    norm_counts = counts.astype(float)\n",
    "    max_count = norm_counts.max()\n",
    "    if max_count > 0:\n",
    "        norm_counts = (norm_counts / max_count) * 255\n",
    "    heatmap = norm_counts.astype(np.uint8)\n",
    "    heatmap_color = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "    heatmap_full = cv2.resize(heatmap_color, (width, height), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    # Overlay heatmap on frame and white background\n",
    "    overlay_frame = cv2.addWeighted(frame, alpha, heatmap_full, beta, 0)\n",
    "    background_white = np.full((height, width, 3), 255, dtype=np.uint8)\n",
    "    overlay_white = cv2.addWeighted(background_white, alpha, heatmap_full, beta, 0)\n",
    "\n",
    "    # Draw grid + zone numbers\n",
    "    for overlay in (overlay_frame, overlay_white):\n",
    "        # Draw grid lines\n",
    "        for i in range(grid_rows + 1):\n",
    "            cv2.line(overlay, (0, i * zone_h), (width, i * zone_h), (255, 255, 255), 1)\n",
    "        for j in range(grid_cols + 1):\n",
    "            cv2.line(overlay, (j * zone_w, 0), (j * zone_w, height), (255, 255, 255), 1)\n",
    "\n",
    "        # Dynamically set font scale based on cell size\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        font_scale = max(min(zone_w, zone_h) / 60.0, 0.3)  # ensures readable but not too large\n",
    "        thickness = 1 if min(zone_w, zone_h) > 25 else 1\n",
    "\n",
    "        # Draw zone numbers at center of each cell\n",
    "        for r in range(grid_rows):\n",
    "            for c in range(grid_cols):\n",
    "                zone_id = r * grid_cols + c + 1\n",
    "                text = str(zone_id)\n",
    "                text_size = cv2.getTextSize(text, font, font_scale, thickness)[0]\n",
    "                tx = c * zone_w + (zone_w - text_size[0]) // 2\n",
    "                ty = r * zone_h + (zone_h + text_size[1]) // 2\n",
    "                color = (0, 0, 0) if overlay is overlay_white else (255, 255, 255)\n",
    "                cv2.putText(overlay, text, (tx, ty), font, font_scale, color, thickness, cv2.LINE_AA)\n",
    "\n",
    "    # Save overlays\n",
    "    cv2.imwrite(output_frame_image, overlay_frame)\n",
    "    cv2.imwrite(output_white_image, overlay_white)\n",
    "\n",
    "    # Display with colorbar\n",
    "    norm = Normalize(vmin=0, vmax=max_count)\n",
    "    mapper = cm.ScalarMappable(norm=norm, cmap='jet')\n",
    "    mapper.set_array([])\n",
    "    for img, title in [(overlay_frame, 'Heatmap on Frame Background'),\n",
    "                       (overlay_white, 'Heatmap on White Background')]:\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.imshow(img_rgb)\n",
    "        plt.title(title)\n",
    "        plt.axis('off')\n",
    "        plt.colorbar(mapper, fraction=0.046, pad=0.04).set_label('Visit Frequency')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def analyze_stalls_and_table():\n",
    "    stall_zones = {\n",
    "        'Stall 1': [51, 52, 61, 62, 71, 72],\n",
    "        'Stall 2': [53, 63, 73],\n",
    "        'Stall 3': [54, 64, 74],\n",
    "        'Stall 4': [56, 57, 66, 67]\n",
    "    }\n",
    "    staff_zones = {51, 52, 53, 54, 56, 61, 62}\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    success, frame = cap.read()\n",
    "    cap.release()\n",
    "\n",
    "    height, width = frame.shape[:2]\n",
    "    zone_h = height // grid_rows\n",
    "    zone_w = width // grid_cols\n",
    "\n",
    "    # Assign zone ID for each log entry\n",
    "    df['zone'] = df.apply(\n",
    "        lambda row: (min(int(row['y']) // zone_h, grid_rows - 1)) * grid_cols +\n",
    "                    min(int(row['x']) // zone_w, grid_cols - 1) + 1,\n",
    "        axis=1\n",
    "    )\n",
    "    df['role'] = df['zone'].apply(lambda z: 'staff' if z in staff_zones else 'customer')\n",
    "\n",
    "    # Table for each stall and role\n",
    "    table = []\n",
    "    for stall, zones in stall_zones.items():\n",
    "        d = df[df['zone'].isin(zones)]\n",
    "        for role in ['customer', 'staff']:\n",
    "            grp = d[d['role'] == role]\n",
    "            total = grp['id'].nunique()\n",
    "            stops = grp.groupby('id').size()\n",
    "            stops_cnt = (stops >= fps * 20).sum()\n",
    "            table.append({'stall': stall, 'role': role, 'total': total, 'stops20s': stops_cnt})\n",
    "    table_df = pd.DataFrame(table)\n",
    "    print(table_df)\n",
    "\n",
    "    # Bar chart\n",
    "    stalls = list(stall_zones.keys())\n",
    "    x = np.arange(len(stalls))\n",
    "    width = 0.35\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    for i, role in enumerate(['customer', 'staff']):\n",
    "        pos = x + (i * 2 - 1) * width / 2\n",
    "        dfr = table_df[table_df['role'] == role]\n",
    "        totals = dfr['total'].values\n",
    "        stops = dfr['stops20s'].values\n",
    "        ax.bar(pos, totals, width, label=f'{role} total',\n",
    "               color=('skyblue' if role == 'customer' else 'lightcoral'))\n",
    "        ax.bar(pos, stops, width, label=f'{role} ≥20s',\n",
    "               color=('blue' if role == 'customer' else 'red'))\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(stalls)\n",
    "    ax.set_xlabel('Stall')\n",
    "    ax.set_ylabel('Unique ID Count')\n",
    "    ax.set_title('Visits per Stall by Role with ≥20s Overlay')\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    create_heatmap_overlay()\n",
    "    analyze_stalls_and_table()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
